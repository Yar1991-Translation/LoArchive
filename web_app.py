"""
Lofter Spider Web Application
ä¸€ä¸ªç°ä»£åŒ–çš„ Lofter çˆ¬è™« Web ç•Œé¢
"""

import os
import sys
import json
import time
import threading
import queue
import re
import requests
from flask import Flask, render_template, request, jsonify, send_from_directory
from flask_cors import CORS

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

app = Flask(__name__, static_folder='static', template_folder='templates')
CORS(app)

# å…¨å±€çŠ¶æ€
task_status = {
    'running': False,
    'current_task': None,
    'progress': 0,
    'message': '',
    'logs': [],
    'error': None
}

# é…ç½®ä¿¡æ¯
config = {
    'login_key': 'LOFTER-PHONE-LOGIN-AUTH',
    'login_auth': '',
    'file_path': './dir',
    'save_path': './dir',  # ç”¨æˆ·è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
    'dark_mode': False,
    'auto_dedup': True,  # è‡ªåŠ¨å»é‡
    'notify_on_complete': True  # å®Œæˆé€šçŸ¥
}

# ä¸‹è½½å†å²æ–‡ä»¶è·¯å¾„
HISTORY_FILE = './download_history.json'
# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = './loarchive_config.json'

def load_config_file():
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    global config
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                saved_config = json.load(f)
                config.update(saved_config)
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    return config

def save_config_file():
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

def load_download_history():
    """åŠ è½½ä¸‹è½½å†å²"""
    default_history = {'items': [], 'stats': {'total': 0, 'images': 0, 'articles': 0}}
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ç¡®ä¿æ•°æ®ç»“æ„å®Œæ•´
                if 'items' not in data:
                    data['items'] = []
                if 'stats' not in data:
                    data['stats'] = {'total': 0, 'images': 0, 'articles': 0}
                return data
        except Exception as e:
            print(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
            return default_history
    return default_history

def save_download_history(history):
    """ä¿å­˜ä¸‹è½½å†å²"""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

def add_to_history(item_type, url, title, author, file_path, source='lofter'):
    """æ·»åŠ åˆ°ä¸‹è½½å†å²"""
    history = load_download_history()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
    for item in history['items']:
        if item.get('url') == url:
            return False  # å·²å­˜åœ¨
    
    # æ·»åŠ æ–°è®°å½•
    record = {
        'id': str(int(time.time() * 1000)),
        'type': item_type,  # 'image', 'article', 'ao3'
        'url': url,
        'title': title,
        'author': author,
        'file_path': file_path,
        'source': source,
        'download_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'timestamp': int(time.time())
    }
    
    history['items'].insert(0, record)  # æœ€æ–°çš„åœ¨å‰é¢
    
    # æ›´æ–°ç»Ÿè®¡
    history['stats']['total'] += 1
    if item_type == 'image':
        history['stats']['images'] += 1
    else:
        history['stats']['articles'] += 1
    
    # é™åˆ¶å†å²è®°å½•æ•°é‡ï¼ˆä¿ç•™æœ€è¿‘1000æ¡ï¼‰
    if len(history['items']) > 1000:
        history['items'] = history['items'][:1000]
    
    save_download_history(history)
    return True

def is_url_downloaded(url):
    """æ£€æŸ¥URLæ˜¯å¦å·²ä¸‹è½½è¿‡"""
    if not config.get('auto_dedup', True):
        return False
    history = load_download_history()
    for item in history['items']:
        if item.get('url') == url:
            return True
    return False

def clear_download_history():
    """æ¸…ç©ºä¸‹è½½å†å²"""
    history = {'items': [], 'stats': {'total': 0, 'images': 0, 'articles': 0}}
    save_download_history(history)
    return True

def load_config():
    """åŠ è½½é…ç½®"""
    global config
    # é¦–å…ˆä»é…ç½®æ–‡ä»¶åŠ è½½
    load_config_file()
    # ç„¶åå°è¯•ä» login_info.py åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        from login_info import login_auth, login_key
        config['login_key'] = login_key
        config['login_auth'] = login_auth
    except:
        pass
    return config

def save_login_info(login_key, login_auth):
    """ä¿å­˜ç™»å½•ä¿¡æ¯åˆ° login_info.py"""
    content = f'''message = """
è¿™ä¸ªæ–‡ä»¶æ˜¯ä¸“é—¨å¡«ç™»å½•ä¿¡æ¯çš„ï¼ŒlofteræŸæ¬¡æ›´æ–°ä¹‹åå¾ˆå¤šé¡µé¢éƒ½è¦ç™»å½•æ‰èƒ½çœ‹ï¼Œæ‰€ä»¥æ¯ä¸ªç¨‹åºéƒ½è¦æœ‰ç™»å½•ä¿¡æ¯æ‰èƒ½ç”¨
åœ¨è¿™é‡Œå¡«å¥½å¯ä»¥åŒæ­¥åˆ°æ¯ä¸ªç¨‹åº
åæ­£å°±æ˜¯ä¸å¡«è¿™é‡Œå…¶ä»–çš„éƒ½è·‘ä¸èµ·æ¥
"""

# ç™»å½•æ–¹å¼å¯¹åº”çš„keyï¼Œè¿™é‡Œé»˜è®¤æ˜¯æ‰‹æœºç™»å½•
login_key = "{login_key}"

# æˆæƒç 
login_auth = "{login_auth}"
'''
    with open('login_info.py', 'w', encoding='utf-8') as f:
        f.write(content)
    
    config['login_key'] = login_key
    config['login_auth'] = login_auth
    
    # é‡æ–°å¯¼å…¥ä»¥æ›´æ–°æ¨¡å—
    import importlib
    try:
        import login_info
        importlib.reload(login_info)
    except:
        pass

def add_log(message):
    """æ·»åŠ æ—¥å¿—"""
    timestamp = time.strftime('%H:%M:%S')
    log_entry = f'[{timestamp}] {message}'
    task_status['logs'].append(log_entry)
    task_status['message'] = message
    print(log_entry)  # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    # ä¿ç•™æœ€æ–°çš„200æ¡æ—¥å¿—
    if len(task_status['logs']) > 200:
        task_status['logs'] = task_status['logs'][-200:]

def run_spider_task(task_type, params):
    """è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    global task_status
    task_status['running'] = True
    task_status['current_task'] = task_type
    task_status['progress'] = 0
    task_status['logs'] = []
    task_status['error'] = None
    
    try:
        load_config()  # é‡æ–°åŠ è½½é…ç½®
        
        # AO3 ä¸éœ€è¦ç™»å½•ï¼Œå…¶ä»–ä»»åŠ¡éœ€è¦
        if task_type == 'ao3':
            run_ao3_task(params)
        elif not config['login_auth']:
            raise Exception("è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®ç™»å½•æˆæƒç ï¼")
        elif task_type == 'single_img':
            run_single_img_task(params)
        elif task_type == 'single_txt':
            run_single_txt_task(params)
        elif task_type == 'author_img':
            run_author_img_task(params)
        elif task_type == 'author_txt':
            run_author_txt_task(params)
        elif task_type == 'like_share_tag':
            run_like_share_tag_task(params)
        else:
            add_log(f'âŒ æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}')
            
    except Exception as e:
        import traceback
        error_msg = str(e)
        task_status['error'] = error_msg
        add_log(f'âŒ ä»»åŠ¡å‡ºé”™: {error_msg}')
        add_log(traceback.format_exc())
    finally:
        task_status['running'] = False
        task_status['progress'] = 100
        add_log('âœ… ä»»åŠ¡ç»“æŸ')


def run_single_img_task(params):
    """è¿è¡Œå•ç¯‡å›¾ç‰‡çˆ¬å–ä»»åŠ¡ - çœŸæ­£è°ƒç”¨ l8_blogs_img.py"""
    import useragentutil
    from lxml.html import etree
    import l4_author_img
    from l13_like_share_tag import filename_check
    
    urls = params.get('urls', [])
    if not urls:
        add_log('âŒ æ²¡æœ‰æä¾›é“¾æ¥')
        return
    
    add_log(f"ğŸš€ å¼€å§‹å•ç¯‡å›¾ç‰‡çˆ¬å–ï¼Œå…± {len(urls)} ä¸ªé“¾æ¥")
    
    login_key = config['login_key']
    login_auth = config['login_auth']
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    save_root = config.get('save_path', './dir')
    dir_path = os.path.join(save_root, "img/this")
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    
    all_imgs_info = []
    
    # è§£ææ¯ä¸ªåšå®¢
    for idx, blog_url in enumerate(urls):
        blog_url = blog_url.strip()
        if not blog_url:
            continue
            
        task_status['progress'] = int((idx / len(urls)) * 50)
        add_log(f"ğŸ“– [{idx+1}/{len(urls)}] è§£æåšå®¢: {blog_url}")
        
        try:
            # è·å–åšå®¢é¡µé¢
            content = requests.get(blog_url, headers=useragentutil.get_headers(),
                                   cookies={login_key: login_auth}).content.decode("utf-8")
            
            # è·å–ä½œè€…ä¿¡æ¯
            author_view_url = blog_url.split("/post")[0] + "/view"
            author_view_html = requests.get(author_view_url, headers=useragentutil.get_headers(),
                                            cookies={login_key: login_auth}).content.decode("utf-8")
            author_view_parse = etree.HTML(author_view_html)
            
            try:
                author_name = author_view_parse.xpath("//h1/a/text()")[0]
            except:
                author_name = "æœªçŸ¥ä½œè€…"
            
            author_ip = re.search(r"http(s)*://(.*).lofter.com/", blog_url).group(2)
            
            # è·å–å‘è¡¨æ—¶é—´
            re_date = re.search(r"\d{4}[.\\\/-]\d{2}[.\\\/-]\d{2}", content)
            if re_date:
                public_time = re_date.group(0).replace("\\", "-").replace(".", "-").replace("/", "-")
            else:
                public_time = time.strftime("%Y-%m-%d")
            
            # åŒ¹é…å›¾ç‰‡é“¾æ¥
            imgs_url = re.findall(r'"(http[s]{0,1}://imglf\d{0,1}.lf\d*.[0-9]{0,3}.net.*?)"', content)
            
            # è¿‡æ»¤å›¾ç‰‡
            filtered_imgs = []
            for img_url in imgs_url:
                if "&amp;" in img_url:
                    continue
                re_url = re.search(r"[1649]{2}[x,y][1649]{2}", img_url)
                if re_url:
                    continue
                img_url = img_url.split("imageView")[0]
                if img_url not in filtered_imgs:
                    filtered_imgs.append(img_url)
            
            add_log(f"   æ‰¾åˆ° {len(filtered_imgs)} å¼ å›¾ç‰‡")
            
            # æ•´ç†å›¾ç‰‡ä¿¡æ¯
            for img_idx, img_url in enumerate(filtered_imgs):
                is_gif = "gif" in img_url
                is_png = "png" in img_url
                if is_gif:
                    img_type = "gif"
                elif is_png:
                    img_type = "png"
                else:
                    img_type = "jpg"
                
                author_name_safe = author_name.replace("/", "&").replace("|", "&").replace("\\", "&").\
                    replace("<", "ã€Š").replace(">", "ã€‹").replace(":", "ï¼š").replace('"', '"').replace("?", "ï¼Ÿ").\
                    replace("*", "Â·").replace("\n", "").replace("(", "ï¼ˆ").replace(")", "ï¼‰")
                
                pic_name = f"{author_name_safe}[{author_ip}] {public_time}({img_idx+1}).{img_type}"
                all_imgs_info.append({
                    "img_url": img_url,
                    "pic_name": pic_name,
                    "referer": blog_url.split("post")[0]
                })
                
        except Exception as e:
            add_log(f"   âš ï¸ è§£æå¤±è´¥: {str(e)}")
            continue
    
    add_log(f"ğŸ“· å…±è·å–åˆ° {len(all_imgs_info)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
    
    # ä¸‹è½½å›¾ç‰‡
    for idx, img_info in enumerate(all_imgs_info):
        task_status['progress'] = 50 + int((idx / len(all_imgs_info)) * 50)
        
        pic_url = img_info["img_url"]
        pic_name = img_info["pic_name"]
        img_path = os.path.join(dir_path, pic_name)
        
        try:
            headers = useragentutil.get_headers()
            headers["Referer"] = img_info.get("referer", "")
            
            response = requests.get(pic_url, headers=headers, timeout=30)
            with open(img_path, "wb") as f:
                f.write(response.content)
            
            add_log(f"   ğŸ’¾ [{idx+1}/{len(all_imgs_info)}] å·²ä¿å­˜: {pic_name}")
            
        except Exception as e:
            add_log(f"   âš ï¸ ä¸‹è½½å¤±è´¥: {pic_name} - {str(e)}")
        
        if idx % 5 == 0:
            time.sleep(0.5)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
    
    add_log(f"âœ… å›¾ç‰‡ä¿å­˜å®Œæˆï¼å…±ä¿å­˜ {len(all_imgs_info)} å¼ å›¾ç‰‡åˆ° {dir_path}")


def run_single_txt_task(params):
    """è¿è¡Œå•ç¯‡æ–‡ç« çˆ¬å–ä»»åŠ¡ - çœŸæ­£è°ƒç”¨ l10_blogs_txt.py"""
    import useragentutil
    from lxml.html import etree
    import l4_author_img
    from l13_like_share_tag import filename_check
    import html2text
    
    urls = params.get('urls', [])
    if not urls:
        add_log('âŒ æ²¡æœ‰æä¾›é“¾æ¥')
        return
    
    add_log(f"ğŸš€ å¼€å§‹å•ç¯‡æ–‡ç« çˆ¬å–ï¼Œå…± {len(urls)} ä¸ªé“¾æ¥")
    
    login_key = config['login_key']
    login_auth = config['login_auth']
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    save_root = config.get('save_path', './dir')
    dir_path = os.path.join(save_root, "article/this")
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    
    saved_count = 0
    
    for idx, blog_url in enumerate(urls):
        blog_url = blog_url.strip()
        if not blog_url:
            continue
            
        task_status['progress'] = int((idx / len(urls)) * 100)
        add_log(f"ğŸ“– [{idx+1}/{len(urls)}] è§£æåšå®¢: {blog_url}")
        
        try:
            # è·å–åšå®¢é¡µé¢
            blog_html = requests.get(blog_url, headers=useragentutil.get_headers(),
                                     cookies={login_key: login_auth}).content.decode("utf-8")
            blog_parse = etree.HTML(blog_html)
            
            # è·å–ä½œè€…ä¿¡æ¯
            author_view_url = blog_url.split("/post")[0] + "/view"
            author_view_html = requests.get(author_view_url, headers=useragentutil.get_headers(),
                                            cookies={login_key: login_auth}).content.decode("utf-8")
            author_view_parse = etree.HTML(author_view_html)
            
            try:
                author_name = author_view_parse.xpath("//h1/a/text()")[0]
            except:
                author_name = "æœªçŸ¥ä½œè€…"
            
            author_ip = re.search(r"http(s)*://(.*).lofter.com/", blog_url).group(2)
            
            # è·å–å‘è¡¨æ—¶é—´
            re_date = re.search(r"\d{4}[.\\\/-]\d{2}[.\\\/-]\d{2}", blog_html)
            if re_date:
                public_time = re_date.group(0).replace("\\", "-").replace(".", "-").replace("/", "-")
            else:
                public_time = time.strftime("%Y-%m-%d")
            
            # è·å–æ ‡é¢˜
            title_path = blog_parse.xpath("//h2//text()")
            if title_path:
                title = title_path[0].strip()
            else:
                title = ""
            
            # è·å–æ­£æ–‡å†…å®¹
            # å°è¯•å¤šç§æ–¹å¼è·å–æ­£æ–‡
            content_text = ""
            
            # æ–¹æ³•1: å°è¯•è·å–æ–‡ç« ä¸»ä½“
            content_elements = blog_parse.xpath("//div[contains(@class,'content')]//text()")
            if content_elements:
                content_text = "\n".join([t.strip() for t in content_elements if t.strip()])
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•è·å–æ‰€æœ‰pæ ‡ç­¾
            if not content_text:
                p_elements = blog_parse.xpath("//article//p//text() | //div[@class='text']//p//text()")
                if p_elements:
                    content_text = "\n\n".join([t.strip() for t in p_elements if t.strip()])
            
            # æ–¹æ³•3: ä½¿ç”¨html2textè½¬æ¢
            if not content_text:
                try:
                    h = html2text.HTML2Text()
                    h.ignore_links = True
                    h.ignore_images = True
                    content_text = h.handle(blog_html)
                    # æ¸…ç†ä¸€äº›æ— ç”¨å†…å®¹
                    content_text = re.sub(r'\n{3,}', '\n\n', content_text)
                except:
                    content_text = "æ— æ³•è§£ææ­£æ–‡å†…å®¹"
            
            # æ„å»ºæ–‡ç« 
            article_head = f"{title if title else 'æ— æ ‡é¢˜'} by {author_name}[{author_ip}]\nå‘è¡¨æ—¶é—´ï¼š{public_time}\nåŸæ–‡é“¾æ¥ï¼š{blog_url}"
            article = article_head + "\n\n" + "="*50 + "\n\n" + content_text
            
            # ç”Ÿæˆæ–‡ä»¶å
            if title:
                file_name = f"{title} by {author_name}.txt"
            else:
                file_name = f"{author_name} {public_time}.txt"
            
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            file_name = file_name.replace("/", "&").replace("|", "&").replace("\\", "&").\
                replace("<", "ã€Š").replace(">", "ã€‹").replace(":", "ï¼š").replace('"', '"').\
                replace("?", "ï¼Ÿ").replace("*", "Â·").replace("\n", "").replace("(", "ï¼ˆ").replace(")", "ï¼‰")
            
            # ä¿å­˜æ–‡ä»¶
            file_path = os.path.join(dir_path, file_name)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(article)
            
            saved_count += 1
            add_log(f"   ğŸ’¾ å·²ä¿å­˜: {file_name}")
            
        except Exception as e:
            add_log(f"   âš ï¸ ä¿å­˜å¤±è´¥: {str(e)}")
            continue
        
        time.sleep(0.5)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
    
    add_log(f"âœ… æ–‡ç« ä¿å­˜å®Œæˆï¼å…±ä¿å­˜ {saved_count} ç¯‡æ–‡ç« åˆ° {dir_path}")


def run_author_img_task(params):
    """è¿è¡Œä½œè€…å›¾ç‰‡çˆ¬å–ä»»åŠ¡"""
    add_log(f"ğŸš€ å¼€å§‹çˆ¬å–ä½œè€…å›¾ç‰‡")
    author_url = params.get('author_url', '')
    
    if not author_url:
        add_log('âŒ è¯·æä¾›ä½œè€…ä¸»é¡µé“¾æ¥')
        return
    
    if not author_url.endswith('/'):
        author_url += '/'
    
    add_log(f"ğŸ“ ä½œè€…ä¸»é¡µ: {author_url}")
    
    try:
        import l4_author_img
        import useragentutil
        from lxml.html import etree
        
        login_key = config['login_key']
        login_auth = config['login_auth']
        
        # è·å–ä½œè€…ä¿¡æ¯
        author_view_url = author_url + "view"
        author_view_html = requests.get(author_view_url, headers=useragentutil.get_headers(),
                                        cookies={login_key: login_auth}).content.decode("utf-8")
        author_page_parse = etree.HTML(author_view_html)
        
        try:
            author_id = author_page_parse.xpath("//body//iframe[@id='control_frame']/@src")[0].split("blogId=")[1]
            author_name = author_page_parse.xpath("//title//text()")[0]
            author_ip = re.search(r"http[s]*://(.*).lofter.com/", author_url).group(1)
            add_log(f"ğŸ‘¤ ä½œè€…: {author_name} ({author_ip})")
        except Exception as e:
            add_log(f"âŒ æ— æ³•è·å–ä½œè€…ä¿¡æ¯: {str(e)}")
            return
        
        # è·å–å½’æ¡£é¡µ
        archive_url = author_url + "dwr/call/plaincall/ArchiveBean.getArchivePostByTime.dwr"
        add_log(f"ğŸ“š æ­£åœ¨è·å–å½’æ¡£é¡µ...")
        
        query_num = 50
        data = l4_author_img.make_data(author_id, query_num)
        header = l4_author_img.make_head(author_url)
        
        all_blog_info = []
        page_num = 0
        
        while True:
            page_num += 1
            add_log(f"   è·å–ç¬¬ {page_num} é¡µ...")
            task_status['progress'] = min(30, page_num * 5)
            
            page_data = l4_author_img.post_content(
                url=archive_url, data=data, head=header,
                cookies_dict={login_key: login_auth}
            )
            
            new_blogs_info = re.findall(r"s[\d]*.blogId.*\n.*noticeLinkTitle", page_data)
            all_blog_info += new_blogs_info
            
            if len(new_blogs_info) < query_num:
                break
            
            try:
                data['c0-param2'] = 'number:' + str(
                    re.search(r's%d\.time=(.*);s.*type' % (query_num - 1), page_data).group(1))
            except:
                break
            
            time.sleep(0.5)
        
        add_log(f"ğŸ“Š å…±è·å– {len(all_blog_info)} æ¡åšå®¢è®°å½•")
        
        # è§£æåšå®¢ä¿¡æ¯ï¼Œè·å–å›¾ç‰‡åšå®¢
        img_blogs = []
        for blog_info in all_blog_info:
            try:
                img_url_match = re.findall(r'[\d]*.imgurl="(.*?)"', blog_info)
                if img_url_match:
                    blog_index = re.search(r's[\d]*.permalink="(.*)"', blog_info).group(1)
                    blog_url = author_url + "post/" + blog_index
                    timestamp = re.search(r's[\d]*.time=(\d*);', blog_info).group(1)
                    dt_time = time.strftime("%Y-%m-%d", time.localtime(int(int(timestamp) / 1000)))
                    img_blogs.append({"url": blog_url, "time": dt_time})
            except:
                continue
        
        add_log(f"ğŸ–¼ï¸ å…±æ‰¾åˆ° {len(img_blogs)} ç¯‡å›¾ç‰‡åšå®¢")
        
        if not img_blogs:
            add_log("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡åšå®¢")
            return
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        author_name_safe = author_name.replace("/", "&").replace("|", "&").replace("\\", "&").\
            replace("<", "ã€Š").replace(">", "ã€‹").replace(":", "ï¼š").replace('"', '"').\
            replace("?", "ï¼Ÿ").replace("*", "Â·").replace("\n", "")
        save_root = config.get('save_path', './dir')
        dir_path = os.path.join(save_root, f"img/{author_name_safe}[{author_ip}]")
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        
        # ä¸‹è½½å›¾ç‰‡
        total_saved = 0
        for idx, blog in enumerate(img_blogs):
            task_status['progress'] = 30 + int((idx / len(img_blogs)) * 70)
            
            try:
                blog_html = requests.get(blog["url"], headers=useragentutil.get_headers(),
                                         cookies={login_key: login_auth}).content.decode("utf-8")
                
                imgs_url = re.findall(r'"(http[s]{0,1}://imglf\d{0,1}.lf\d*.[0-9]{0,3}.net.*?)"', blog_html)
                
                # è¿‡æ»¤
                filtered_imgs = []
                for img_url in imgs_url:
                    if "&amp;" in img_url:
                        continue
                    re_url = re.search(r"[1649]{2}[x,y][1649]{2}", img_url)
                    if re_url:
                        continue
                    img_url = img_url.split("imageView")[0]
                    if img_url not in filtered_imgs:
                        filtered_imgs.append(img_url)
                
                for img_idx, img_url in enumerate(filtered_imgs):
                    is_gif = "gif" in img_url
                    is_png = "png" in img_url
                    img_type = "gif" if is_gif else ("png" if is_png else "jpg")
                    
                    pic_name = f"{author_name_safe}[{author_ip}] {blog['time']}({img_idx+1}).{img_type}"
                    img_path = os.path.join(dir_path, pic_name)
                    
                    headers = useragentutil.get_headers()
                    headers["Referer"] = author_url
                    
                    img_content = requests.get(img_url, headers=headers, timeout=30).content
                    with open(img_path, "wb") as f:
                        f.write(img_content)
                    
                    total_saved += 1
                
                if idx % 10 == 0:
                    add_log(f"   ğŸ“¥ è¿›åº¦: {idx+1}/{len(img_blogs)} åšå®¢, å·²ä¿å­˜ {total_saved} å¼ å›¾ç‰‡")
                    
            except Exception as e:
                add_log(f"   âš ï¸ å¤„ç†åšå®¢å¤±è´¥: {blog['url']} - {str(e)}")
                continue
            
            time.sleep(0.3)
        
        add_log(f"âœ… å®Œæˆï¼å…±ä¿å­˜ {total_saved} å¼ å›¾ç‰‡åˆ° {dir_path}")
        
    except Exception as e:
        import traceback
        add_log(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
        add_log(traceback.format_exc())


def run_author_txt_task(params):
    """è¿è¡Œä½œè€…æ–‡ç« çˆ¬å–ä»»åŠ¡"""
    add_log(f"ğŸš€ å¼€å§‹çˆ¬å–ä½œè€…æ–‡ç« ")
    add_log("âš ï¸ æ­¤åŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼Œè¯·ä½¿ç”¨å•ç¯‡ä¿å­˜åŠŸèƒ½")
    # TODO: å®Œæ•´å®ç°


def run_like_share_tag_task(params):
    """è¿è¡Œå–œæ¬¢/æ¨è/Tagçˆ¬å–ä»»åŠ¡"""
    import useragentutil
    from lxml.html import etree
    from urllib import parse as url_parse
    import html2text
    
    url = params.get('url', '')
    mode = params.get('mode', 'like2')  # like1, like2, share, tag
    save_mode = params.get('save_mode', {"article": 1, "text": 1, "long article": 1, "img": 1})
    export_pdf = params.get('export_pdf', False)  # æ˜¯å¦å¯¼å‡ºPDF
    
    # Lofteræ–‡ç« PDFç”Ÿæˆå‡½æ•°
    def generate_lofter_pdf(title, author, author_ip, public_time, url, content, pdf_path):
        """ä¸ºLofteræ–‡ç« ç”ŸæˆPDF"""
        try:
            from xhtml2pdf import pisa
            from reportlab.pdfbase import pdfmetrics
            from reportlab.pdfbase.cidfonts import UnicodeCIDFont
            import datetime
            
            # æ³¨å†Œä¸­æ–‡å­—ä½“
            try:
                pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
            except:
                pass
            
            # å¤„ç†å†…å®¹ä¸­çš„æ¢è¡Œ
            content_html = content.replace('\n', '<br/>')
            current_date = datetime.datetime.now().strftime("%Y-%m-%d")
            
            html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>{title} - {author}</title>
    <style>
        @page {{ 
            size: A4; 
            margin: 2.5cm 2cm; 
            @bottom-center {{
                content: counter(page);
                font-size: 10pt;
                color: #666;
            }}
        }}
        
        body {{ font-family: STSong-Light, SimSun, serif; font-size: 12pt; line-height: 1.8; color: #333; }}
        
        /* å°é¢æ ·å¼ */
        .cover {{
            text-align: center;
            padding-top: 20%;
            page-break-after: always;
            height: 100%;
        }}
        
        .cover-title {{
            font-size: 28pt;
            font-weight: bold;
            margin-bottom: 30px;
            color: #2c3e50;
        }}
        
        .cover-author {{
            font-size: 16pt;
            margin-bottom: 60px;
            color: #555;
        }}
        
        .cover-meta {{
            font-size: 11pt;
            color: #7f8c8d;
            margin-top: 100px;
            border-top: 1px solid #ddd;
            padding-top: 30px;
            width: 60%;
            margin-left: auto;
            margin-right: auto;
        }}
        
        /* æ­£æ–‡å†…å®¹ */
        .content-title {{
            font-size: 18pt;
            font-weight: bold;
            text-align: center;
            margin-bottom: 10px;
            padding-top: 30px;
        }}
        
        .content-meta {{
            font-size: 10pt;
            color: #999;
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 1px solid #eee;
            padding-bottom: 20px;
        }}
        
        .content {{ text-align: justify; }}
        .content p {{ text-indent: 2em; margin-bottom: 10px; }}
        
        a {{ color: #3498db; text-decoration: none; }}
    </style>
</head>
<body>
    <!-- å°é¢ -->
    <div class="cover">
        <div class="cover-title">{title or "æ— æ ‡é¢˜"}</div>
        <div class="cover-author">By {author}</div>
        
        <div class="cover-meta">
            <div>Published: {public_time}</div>
            <div>Source: Lofter</div>
            <div style="margin-top: 20px; font-size: 10pt;">
                Generated on {current_date}
            </div>
        </div>
    </div>

    <!-- æ­£æ–‡ -->
    <div class="content-title">{title or "æ— æ ‡é¢˜"}</div>
    <div class="content-meta">
        ä½œè€…: {author} [{author_ip}] &nbsp;|&nbsp; æ—¶é—´: {public_time}<br/>
        åŸæ–‡: {url}
    </div>
    
    <div class="content">
        <p>{content_html}</p>
    </div>
</body>
</html>'''
            
            with open(pdf_path, 'wb') as pdf_file:
                pisa.CreatePDF(html_content.encode('utf-8'), dest=pdf_file, encoding='utf-8')
            return True
        except Exception as e:
            return False
    
    if not url:
        add_log('âŒ è¯·æä¾›é“¾æ¥åœ°å€')
        return
    
    add_log(f"ğŸš€ å¼€å§‹ {mode} æ¨¡å¼çˆ¬å–ä»»åŠ¡")
    add_log(f"ğŸ“ URL: {url}")
    
    login_key = config['login_key']
    login_auth = config['login_auth']
    
    try:
        # è·å–ç™»å½•session
        add_log("ğŸ” æ­£åœ¨å»ºç«‹ç™»å½•ä¼šè¯...")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Host": "www.lofter.com",
        }
        
        session = requests.session()
        session.headers = headers
        session.cookies.set(login_key, login_auth)
        
        # æ ¹æ®æ¨¡å¼ç¡®å®šè¯·æ±‚URLå’Œå‚æ•°
        if mode == "like2":
            requests_url = "http://www.lofter.com/dwr/call/plaincall/PostBean.getFavTrackItem.dwr"
            headers["Referer"] = "http://www.lofter.com/like"
        elif mode == "like1":
            requests_url = "https://www.lofter.com/dwr/call/plaincall/BlogBean.queryLikePosts.dwr"
            userName = re.search(r"http[s]{0,1}://(.*?).lofter.com/", url).group(1)
            headers["Referer"] = "https://www.lofter.com/favblog/" + userName
        elif mode == "share":
            requests_url = "https://www.lofter.com/dwr/call/plaincall/BlogBean.querySharePosts.dwr"
            userName = re.search(r"http[s]{0,1}://(.*?).lofter.com/", url).group(1)
            headers["Referer"] = "https://www.lofter.com/shareblog/" + userName
        elif mode == "tag":
            requests_url = "http://www.lofter.com/dwr/call/plaincall/TagBean.search.dwr"
            headers["Referer"] = url
        else:
            add_log(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
            return
        
        session.headers = headers
        
        # è·å–ç”¨æˆ·ID (like1, share æ¨¡å¼éœ€è¦)
        userId = ""
        if mode in ["like1", "share"]:
            add_log("ğŸ“– è·å–ç”¨æˆ·ä¿¡æ¯...")
            host = re.search(r"https://(.*?)/", url).group(1)
            session.headers["Host"] = host
            user_page = session.get(url).content.decode("utf-8")
            user_page_parse = etree.HTML(user_page)
            try:
                userId = user_page_parse.xpath("//body/iframe[@id='control_frame']/@src")[0].split("blogId=")[1]
                add_log(f"   ç”¨æˆ·ID: {userId}")
            except:
                add_log("âŒ æ— æ³•è·å–ç”¨æˆ·IDï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æ­£ç¡®")
                return
            session.headers["Host"] = "www.lofter.com"
        
        # æ„å»ºåˆå§‹è¯·æ±‚å‚æ•°
        base_data = {
            'callCount': '1',
            'httpSessionId': '',
            'scriptSessionId': '${scriptSessionId}187',
            'c0-id': '0',
            "batchId": "472351"
        }
        
        get_num = 100
        got_num = 0
        
        if mode in ["like1", "share"]:
            data_params = {
                'c0-scriptName': 'BlogBean',
                "c0-methodName": "queryLikePosts" if mode == "like1" else "querySharePosts",
                'c0-param0': 'number:' + str(userId),
                'c0-param1': 'number:' + str(get_num),
                'c0-param2': 'number:' + str(got_num),
                'c0-param3': 'string:'
            }
        elif mode == "like2":
            data_params = {
                "c0-scriptName": "PostBean",
                "c0-methodName": "getFavTrackItem",
                "c0-param0": "number:" + str(get_num),
                "c0-param1": "number:" + str(got_num),
            }
        elif mode == "tag":
            url_search = re.search(r"http[s]{0,1}://www.lofter.com/tag/(.*?)/(.*)", url)
            if url_search:
                tag_name = url_search.group(1)
                tag_type = url_search.group(2) if url_search.group(2) else "new"
            else:
                url_search = re.search(r"http[s]{0,1}://www.lofter.com/tag/(.*)", url)
                tag_name = url_search.group(1) if url_search else ""
                tag_type = "new"
            
            data_params = {
                'c0-scriptName': 'TagBean',
                'c0-methodName': 'search',
                'c0-param0': 'string:' + tag_name,
                'c0-param1': 'number:0',
                'c0-param2': 'string:',
                'c0-param3': 'string:' + tag_type,
                'c0-param4': 'boolean:false',
                'c0-param5': 'number:0',
                'c0-param6': 'number:' + str(get_num),
                'c0-param7': 'number:' + str(got_num),
                'c0-param8': 'number:' + str(int(time.time() * 1000)),
                'batchId': '870178'
            }
        
        data = {**base_data, **data_params}
        
        # å¼€å§‹è·å–æ•°æ®
        add_log("ğŸ“¥ å¼€å§‹è·å–æ•°æ®...")
        all_fav_info = []
        real_got_num = 0
        
        while True:
            add_log(f"   è¯·æ±‚ {got_num}-{got_num + get_num}...")
            task_status['progress'] = min(30, int(got_num / 10))
            
            response = session.post(requests_url, data=data)
            content = response.content.decode("utf-8")
            
            # æŒ‰ activityTags åˆ‡åˆ†
            new_info = content.split("activityTags")[1:]
            all_fav_info += new_info
            got_num += get_num
            real_got_num += len(new_info)
            
            add_log(f"   å®é™…è¿”å› {len(new_info)} æ¡")
            
            if len(new_info) == 0:
                add_log("   å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break
            
            if got_num >= 500:  # é™åˆ¶è·å–æ•°é‡ï¼Œé¿å…å¤ªæ…¢
                add_log("   å·²è¾¾åˆ°500æ¡é™åˆ¶")
                break
            
            # æ›´æ–°è¯·æ±‚å‚æ•°
            if mode in ["like1", "share"]:
                data["c0-param1"] = 'number:' + str(get_num)
                data["c0-param2"] = 'number:' + str(got_num)
            elif mode == "like2":
                data["c0-param0"] = 'number:' + str(get_num)
                data["c0-param1"] = 'number:' + str(got_num)
            elif mode == "tag":
                try:
                    last_info = new_info[-1]
                    last_timestamp = re.search(r's\d{1,5}.publishTime=(.*?);', last_info).group(1)
                    data["c0-param6"] = 'number:' + str(get_num)
                    data["c0-param7"] = 'number:' + str(got_num)
                    data["c0-param8"] = 'number:' + str(last_timestamp)
                except:
                    break
            
            time.sleep(0.5)
        
        add_log(f"ğŸ“Š å…±è·å–åˆ° {real_got_num} æ¡åšå®¢ä¿¡æ¯")
        
        if real_got_num == 0:
            add_log("âš ï¸ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç™»å½•ä¿¡æ¯å’Œé“¾æ¥")
            return
        
        # è§£æåšå®¢ä¿¡æ¯
        add_log("ğŸ”„ æ­£åœ¨è§£æåšå®¢ä¿¡æ¯...")
        blogs_info = []
        
        for idx, fav_info in enumerate(all_fav_info):
            try:
                # åšå®¢é“¾æ¥
                blog_url = re.search(r's\d{1,5}.blogPageUrl="(.*?)"', fav_info)
                if not blog_url:
                    continue
                blog_url = blog_url.group(1)
                
                # ä½œè€…å
                author_name_search = re.search(r's\d{1,5}.blogNickName="(.*?)"', fav_info)
                if author_name_search:
                    author_name = author_name_search.group(1).encode('latin-1').decode('unicode_escape', errors="replace")
                else:
                    author_name = "æœªçŸ¥ä½œè€…"
                
                # ä½œè€…IP
                author_ip = re.search(r"http[s]{0,1}://(.*?).lofter.com", blog_url).group(1)
                
                # å‘è¡¨æ—¶é—´
                public_timestamp = re.search(r's\d{1,5}.publishTime=(.*?);', fav_info)
                if public_timestamp:
                    time_local = time.localtime(int(int(public_timestamp.group(1)) / 1000))
                    public_time = time.strftime("%Y-%m-%d", time_local)
                else:
                    public_time = "æœªçŸ¥æ—¶é—´"
                
                # å›¾ç‰‡é“¾æ¥
                img_urls = []
                urls_search = re.search(r'originPhotoLinks="(\[.*?\])"', fav_info)
                if urls_search:
                    try:
                        urls_str = urls_search.group(1).replace("\\", "").replace("false", "False").replace("true", "True")
                        urls_infos = eval(urls_str)
                        for url_info in urls_infos:
                            img_url = url_info.get("raw", "") or url_info.get("orign", "").split("?imageView")[0]
                            if img_url:
                                img_urls.append(img_url)
                    except:
                        pass
                
                # æ­£æ–‡å†…å®¹
                content_search = re.search(r's\d{1,5}.content="(.*?)";', fav_info)
                if content_search:
                    content = content_search.group(1).encode('latin-1').decode("unicode_escape", errors="ignore")
                    try:
                        h = html2text.HTML2Text()
                        h.ignore_links = False
                        content = h.handle(content)
                    except:
                        pass
                else:
                    content = ""
                
                # æ ‡é¢˜
                title_search = re.search(r's\d{1,5}.title="(.*?)"', fav_info)
                title = ""
                if title_search:
                    title = title_search.group(1).encode('latin-1').decode('unicode_escape', errors="ignore")
                
                blogs_info.append({
                    "url": blog_url,
                    "author_name": author_name,
                    "author_ip": author_ip,
                    "public_time": public_time,
                    "img_urls": img_urls,
                    "content": content,
                    "title": title,
                    "has_img": len(img_urls) > 0
                })
                
            except Exception as e:
                continue
        
        add_log(f"âœ… è§£æå®Œæˆï¼Œå…± {len(blogs_info)} æ¡æœ‰æ•ˆåšå®¢")
        
        # ä¿å­˜å†…å®¹
        img_count = sum(1 for b in blogs_info if b["has_img"])
        txt_count = sum(1 for b in blogs_info if not b["has_img"])
        add_log(f"ğŸ“Š å›¾ç‰‡åšå®¢: {img_count} ç¯‡, æ–‡å­—åšå®¢: {txt_count} ç¯‡")
        
        # åˆ›å»ºä¿å­˜ç›®å½• - æŒ‰ä½œè€…åˆ†ç±»
        save_root = config.get('save_path', './dir')
        base_dir = os.path.join(save_root, f"{mode}_save")
        img_base_dir = os.path.join(base_dir, "img")
        txt_base_dir = os.path.join(base_dir, "txt")
        os.makedirs(img_base_dir, exist_ok=True)
        os.makedirs(txt_base_dir, exist_ok=True)
        
        saved_img = 0
        saved_txt = 0
        
        # ç”¨äºå®‰å…¨åŒ–æ–‡ä»¶åçš„å‡½æ•°
        def safe_name(name):
            return name.replace("/", "&").replace("\\", "&").replace(":", "ï¼š").\
                replace("*", "Â·").replace("?", "ï¼Ÿ").replace('"', "'").\
                replace("<", "ã€Š").replace(">", "ã€‹").replace("|", "&").\
                replace("\n", "").replace("\r", "").replace("\t", " ").strip()
        
        for idx, blog in enumerate(blogs_info):
            task_status['progress'] = 30 + int((idx / len(blogs_info)) * 70)
            
            try:
                # ç”Ÿæˆä½œè€…ç›®å½•å
                author_safe = safe_name(blog["author_name"])
                author_folder = f"{author_safe}[{blog['author_ip']}]"
                
                # ä¿å­˜å›¾ç‰‡ - æŒ‰ä½œè€…åˆ†ç±»
                if blog["has_img"] and save_mode.get("img"):
                    # åˆ›å»ºä½œè€…ä¸“å±å›¾ç‰‡ç›®å½•
                    author_img_dir = os.path.join(img_base_dir, author_folder)
                    os.makedirs(author_img_dir, exist_ok=True)
                    
                    for img_idx, img_url in enumerate(blog["img_urls"]):
                        try:
                            # ç¡®å®šå›¾ç‰‡ç±»å‹
                            img_type = "gif" if "gif" in img_url else ("png" if "png" in img_url else "jpg")
                            
                            pic_name = f"{blog['public_time']}({img_idx+1}).{img_type}"
                            img_path = os.path.join(author_img_dir, pic_name)
                            
                            headers = useragentutil.get_headers()
                            headers["Referer"] = blog["url"].split("post")[0]
                            
                            img_content = requests.get(img_url, headers=headers, timeout=30).content
                            with open(img_path, "wb") as f:
                                f.write(img_content)
                            
                            saved_img += 1
                        except:
                            continue
                
                # ä¿å­˜æ–‡ç« /æ–‡æœ¬ - æŒ‰ä½œè€…åˆ†ç±»
                if (blog["title"] and save_mode.get("article")) or (not blog["title"] and save_mode.get("text")):
                    # åˆ›å»ºä½œè€…ä¸“å±æ–‡ç« ç›®å½•
                    author_txt_dir = os.path.join(txt_base_dir, author_folder)
                    os.makedirs(author_txt_dir, exist_ok=True)
                    
                    if blog["title"]:
                        title_safe = safe_name(blog["title"])
                        file_name = f"{title_safe}.txt"
                    else:
                        file_name = f"{blog['public_time']}.txt"
                    
                    txt_path = os.path.join(author_txt_dir, file_name)
                    
                    # é¿å…æ–‡ä»¶åé‡å¤
                    counter = 1
                    original_path = txt_path
                    while os.path.exists(txt_path):
                        name_part = original_path.rsplit('.', 1)[0]
                        txt_path = f"{name_part}({counter}).txt"
                        counter += 1
                    
                    article_head = f"{blog['title'] or 'æ— æ ‡é¢˜'} by {blog['author_name']}[{blog['author_ip']}]\n"
                    article_head += f"å‘è¡¨æ—¶é—´ï¼š{blog['public_time']}\nåŸæ–‡é“¾æ¥ï¼š{blog['url']}\n"
                    article_head += "=" * 50 + "\n\n"
                    
                    article = article_head + blog["content"]
                    
                    with open(txt_path, "w", encoding="utf-8") as f:
                        f.write(article)
                    
                    # å¦‚æœéœ€è¦ç”ŸæˆPDF
                    if export_pdf:
                        pdf_path = txt_path.replace('.txt', '.pdf')
                        generate_lofter_pdf(
                            title=blog['title'] or 'æ— æ ‡é¢˜',
                            author=blog['author_name'],
                            author_ip=blog['author_ip'],
                            public_time=blog['public_time'],
                            url=blog['url'],
                            content=blog['content'],
                            pdf_path=pdf_path
                        )
                    
                    saved_txt += 1
                
                if idx % 20 == 0:
                    add_log(f"   è¿›åº¦: {idx+1}/{len(blogs_info)}, å·²ä¿å­˜å›¾ç‰‡ {saved_img} å¼ , æ–‡ç«  {saved_txt} ç¯‡")
                    
            except Exception as e:
                continue
            
            time.sleep(0.1)
        
        add_log(f"âœ… ä¿å­˜å®Œæˆï¼ï¼ˆæ–‡ä»¶æŒ‰ä½œè€…åˆ†ç±»å­˜æ”¾ï¼‰")
        add_log(f"   ğŸ“· å›¾ç‰‡: {saved_img} å¼  â†’ {img_base_dir}/ä½œè€…å/")
        add_log(f"   ğŸ“ æ–‡ç« : {saved_txt} ç¯‡ â†’ {txt_base_dir}/ä½œè€…å/")
        
    except Exception as e:
        import traceback
        add_log(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
        add_log(traceback.format_exc())


def generate_epub(title, author, content_parts, chapters_info, metadata_list, filepath):
    """ç”Ÿæˆ EPUB ç”µå­ä¹¦"""
    try:
        from ebooklib import epub
        import uuid
        
        book = epub.EpubBook()
        
        # è®¾ç½®å…ƒæ•°æ®
        book.set_identifier(str(uuid.uuid4()))
        book.set_title(title)
        book.set_language('zh')
        book.add_author(author)
        
        # æ·»åŠ  CSS æ ·å¼
        style = '''
        body { font-family: "Noto Serif SC", "Source Han Serif", serif; line-height: 1.8; margin: 2em; }
        h1 { text-align: center; margin-bottom: 1em; }
        h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5em; margin-top: 2em; }
        p { text-indent: 2em; margin-bottom: 0.5em; }
        .meta { font-size: 0.9em; color: #666; margin-bottom: 2em; padding: 1em; background: #f5f5f5; border-radius: 5px; }
        .meta-item { margin-bottom: 0.3em; }
        '''
        css = epub.EpubItem(uid="style", file_name="style/main.css", media_type="text/css", content=style)
        book.add_item(css)
        
        chapters = []
        
        # å°é¢/å…ƒæ•°æ®é¡µ
        if metadata_list:
            cover_content = f'<html><head><link rel="stylesheet" href="style/main.css"/></head><body>'
            cover_content += f'<h1>{title}</h1>'
            cover_content += f'<p style="text-align:center;">by {author}</p>'
            cover_content += '<div class="meta">'
            for meta in metadata_list:
                if meta.strip():
                    cover_content += f'<div class="meta-item">{meta}</div>'
            cover_content += '</div></body></html>'
            
            cover_chapter = epub.EpubHtml(title='ä½œå“ä¿¡æ¯', file_name='cover.xhtml', lang='zh')
            cover_chapter.content = cover_content
            cover_chapter.add_item(css)
            book.add_item(cover_chapter)
            chapters.append(cover_chapter)
        
        # å†…å®¹ç« èŠ‚
        if chapters_info:
            for idx, (ch_title, ch_content) in enumerate(chapters_info):
                ch = epub.EpubHtml(title=ch_title, file_name=f'chapter_{idx+1}.xhtml', lang='zh')
                content = f'<html><head><link rel="stylesheet" href="style/main.css"/></head><body>'
                content += f'<h2>{ch_title}</h2>'
                for para in ch_content:
                    if para.strip():
                        content += f'<p>{para}</p>'
                content += '</body></html>'
                ch.content = content
                ch.add_item(css)
                book.add_item(ch)
                chapters.append(ch)
        else:
            # å•ç« èŠ‚
            main_ch = epub.EpubHtml(title='æ­£æ–‡', file_name='content.xhtml', lang='zh')
            content = f'<html><head><link rel="stylesheet" href="style/main.css"/></head><body>'
            content += f'<h1>{title}</h1>'
            for para in content_parts:
                if para.strip() and not para.strip().startswith('='*10):
                    content += f'<p>{para}</p>'
            content += '</body></html>'
            main_ch.content = content
            main_ch.add_item(css)
            book.add_item(main_ch)
            chapters.append(main_ch)
        
        # ç›®å½•
        book.toc = chapters
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        book.spine = ['nav'] + chapters
        
        # ä¿å­˜
        epub.write_epub(filepath, book)
        return True
    except Exception as e:
        print(f"EPUBç”Ÿæˆå¤±è´¥: {e}")
        return False


def run_ao3_task(params):
    """è¿è¡ŒAO3æ–‡ç« çˆ¬å–ä»»åŠ¡ - å‚è€ƒ https://github.com/610yilingliu/download_ao3_v2"""
    from lxml import etree
    from bs4 import BeautifulSoup
    import html as html_module
    
    urls = params.get('urls', [])
    mode = params.get('mode', 'work')  # work, series, author, tag
    download_chapters = params.get('download_chapters', True)
    save_metadata = params.get('save_metadata', True)
    export_pdf = params.get('export_pdf', False)  # æ˜¯å¦å¯¼å‡ºPDF
    export_epub = params.get('export_epub', False)  # æ˜¯å¦å¯¼å‡ºEPUB
    
    # PDFç”Ÿæˆçš„HTMLæ¨¡æ¿
    def generate_html_content(title, author, work_url, metadata_list, content_parts, chapters_info=None):
        """ç”Ÿæˆç¾åŒ–çš„HTMLå†…å®¹ - ä¹¦ç±é£æ ¼"""
        import datetime
        current_date = datetime.datetime.now().strftime("%Y-%m-%d")
        
        # å¤„ç†å…ƒæ•°æ®
        meta_html = ""
        fandom = ""
        rating = ""
        
        if metadata_list:
            for meta in metadata_list:
                meta = meta.strip()
                if not meta: continue
                
                # æå–å…³é”®ä¿¡æ¯ç”¨äºå°é¢
                if meta.startswith("Fandom:"):
                    fandom = meta.split(":", 1)[1].strip()
                elif meta.startswith("Rating:"):
                    rating = meta.split(":", 1)[1].strip()
                
                if ':' in meta:
                    key, value = meta.split(':', 1)
                    meta_html += f'<div class="meta-item"><span class="meta-label">{key.strip()}:</span> <span class="meta-value">{value.strip()}</span></div>\n'
                else:
                    meta_html += f'<div class="meta-item">{meta}</div>\n'
        
        # å¤„ç†æ­£æ–‡å†…å®¹
        content_html = ""
        if chapters_info:
            # å¤šç« èŠ‚
            for i, (ch_title, ch_content) in enumerate(chapters_info):
                content_html += f'<div class="chapter">\n'
                content_html += f'<h2 class="chapter-title">{ch_title}</h2>\n'
                for para in ch_content:
                    if para.strip():
                        content_html += f'<p>{para}</p>\n'
                content_html += '</div>\n'
                # ç« èŠ‚ç»“æŸåæ·»åŠ åˆ†é¡µç¬¦ï¼ˆé™¤æœ€åä¸€ç« å¤–ï¼‰
                if i < len(chapters_info) - 1:
                    content_html += '<div class="page-break"></div>\n'
        else:
            # å•ç« èŠ‚
            content_html += '<div class="chapter">\n'
            for para in content_parts:
                # è¿‡æ»¤æ‰TXTæ ¼å¼çš„åˆ†éš”ç¬¦
                if para.strip() and not para.strip().startswith('='*10):
                    content_html += f'<p>{para}</p>\n'
            content_html += '</div>\n'
        
        html_template = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>{title} - {author}</title>
    <style>
        @page {{
            size: A4;
            margin: 2.5cm 2cm;
            @bottom-center {{
                content: counter(page);
                font-size: 10pt;
                color: #666;
            }}
        }}
        
        body {{
            font-family: STSong-Light, SimSun, serif;
            font-size: 12pt;
            line-height: 1.8;
            color: #222;
            background: #fff;
        }}
        
        /* å°é¢æ ·å¼ */
        .cover {{
            text-align: center;
            padding-top: 15%;
            page-break-after: always;
            height: 100%;
        }}
        
        .cover-title {{
            font-size: 32pt;
            font-weight: bold;
            margin-bottom: 30px;
            color: #2c3e50;
            line-height: 1.3;
        }}
        
        .cover-author {{
            font-size: 18pt;
            margin-bottom: 60px;
            color: #555;
        }}
        
        .cover-meta {{
            font-size: 12pt;
            color: #7f8c8d;
            margin-top: 100px;
            border-top: 1px solid #ddd;
            padding-top: 30px;
            width: 60%;
            margin-left: auto;
            margin-right: auto;
        }}
        
        .cover-fandom {{
            font-style: italic;
            margin-bottom: 10px;
        }}
        
        /* å…ƒæ•°æ®é¡µ */
        .metadata-page {{
            page-break-after: always;
            padding: 2cm 0;
        }}
        
        .metadata-title {{
            font-size: 18pt;
            border-bottom: 2px solid #8B4513;
            padding-bottom: 10px;
            margin-bottom: 30px;
            color: #8B4513;
        }}
        
        .metadata-content {{
            background-color: #fafafa;
            padding: 20px;
            border: 1px solid #eee;
            border-radius: 5px;
        }}
        
        .meta-item {{
            margin-bottom: 8px;
            font-size: 11pt;
        }}
        
        .meta-label {{
            font-weight: bold;
            color: #555;
        }}
        
        /* æ­£æ–‡æ ·å¼ */
        .content {{
            text-align: justify;
        }}
        
        .chapter-title {{
            font-size: 18pt;
            font-weight: bold;
            text-align: center;
            margin: 40px 0 30px 0;
            color: #2c3e50;
        }}
        
        p {{
            text-indent: 2em;
            margin-bottom: 12px;
            line-height: 1.8;
        }}
        
        .page-break {{
            page-break-after: always;
        }}
        
        a {{ color: #3498db; text-decoration: none; }}
    </style>
</head>
<body>
    <!-- å°é¢é¡µ -->
    <div class="cover">
        <div class="cover-title">{title}</div>
        <div class="cover-author">By {author}</div>
        
        <div class="cover-meta">
            {f'<div class="cover-fandom">{fandom}</div>' if fandom else ''}
            <div>Rating: {rating or 'Not Rated'}</div>
            <div style="margin-top: 20px; font-size: 10pt;">
                Generated by Lofter Spider<br/>
                {current_date}
            </div>
        </div>
    </div>
    
    <!-- å…ƒæ•°æ®é¡µ -->
    <div class="metadata-page">
        <div class="metadata-title">Work Details</div>
        <div class="metadata-content">
            {meta_html}
            <div class="meta-item" style="margin-top: 20px; border-top: 1px dashed #ccc; padding-top: 10px;">
                <span class="meta-label">Original URL:</span> 
                <span class="meta-value">{work_url}</span>
            </div>
        </div>
    </div>
    
    <!-- æ­£æ–‡å†…å®¹ -->
    <div class="content">
        {content_html}
    </div>
</body>
</html>'''
        return html_template
    
    def save_as_pdf(html_content, filepath):
        """å°†HTMLå†…å®¹ä¿å­˜ä¸ºPDF - ä½¿ç”¨xhtml2pdfï¼Œæ”¯æŒä¸­æ–‡"""
        try:
            from xhtml2pdf import pisa
            from reportlab.pdfbase import pdfmetrics
            from reportlab.pdfbase.cidfonts import UnicodeCIDFont
            
            # æ³¨å†Œä¸­æ–‡CIDå­—ä½“ (ReportLabå†…ç½®ï¼Œæ”¯æŒç®€ä½“ä¸­æ–‡)
            try:
                pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
            except Exception:
                pass
            
            with open(filepath, 'wb') as pdf_file:
                pisa_status = pisa.CreatePDF(
                    html_content.encode('utf-8'),
                    dest=pdf_file,
                    encoding='utf-8'
                )
            
            if pisa_status.err:
                add_log(f"   âš ï¸ PDFç”Ÿæˆæœ‰è­¦å‘Šï¼Œä½†æ–‡ä»¶å·²åˆ›å»º")
            return True
            
        except Exception as e:
            add_log(f"   âš ï¸ PDFç”Ÿæˆå¤±è´¥: {str(e)}")
            return False
    
    if not urls:
        add_log('âŒ è¯·æä¾›AO3é“¾æ¥')
        return
    
    add_log(f"ğŸ“š å¼€å§‹AO3çˆ¬å–ä»»åŠ¡ï¼Œæ¨¡å¼: {mode}")
    add_log(f"ğŸ“ å…± {len(urls)} ä¸ªé“¾æ¥")
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„ï¼‰
    save_root = config.get('save_path', './dir')
    base_dir = os.path.join(save_root, 'ao3')
    os.makedirs(base_dir, exist_ok=True)
    
    # AO3è¯·æ±‚Session - æ›´å¥½çš„è¿æ¥ç®¡ç†
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    })
    # è®¾ç½®cookieç»•è¿‡å¹´é¾„ç¡®è®¤
    session.cookies.set('accepted_tos', '20180523', domain='.archiveofourown.org')
    session.cookies.set('view_adult', 'true', domain='.archiveofourown.org')
    
    # ç”¨äºå…¼å®¹æ—§ä»£ç çš„headerså˜é‡
    ao3_headers = session.headers
    
    saved_count = 0
    
    def safe_filename(name):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # Windowséæ³•å­—ç¬¦
        invalid_chars = r'[\\/*?:"<>|\r\n\t]'
        name = re.sub(invalid_chars, '_', name).strip()
        # ç§»é™¤è¿ç»­ç©ºæ ¼å’Œä¸‹åˆ’çº¿
        name = re.sub(r'[_\s]+', ' ', name).strip()
        return name[:100] if name else "untitled"
    
    def fetch_with_retry(url, max_retries=3, wait_time=30):
        """å¸¦é‡è¯•é€»è¾‘çš„è¯·æ±‚å‡½æ•°"""
        for attempt in range(max_retries):
            try:
                response = session.get(url, timeout=30)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:
                    # è¯·æ±‚è¿‡äºé¢‘ç¹
                    add_log(f"   âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹(429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code == 404:
                    add_log(f"   âš ï¸ ä½œå“ä¸å­˜åœ¨æˆ–å·²åˆ é™¤ (404)")
                    return None
                elif response.status_code == 403:
                    add_log(f"   âš ï¸ æ— æƒè®¿é—® (403)ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–ä½œå“å·²é”å®š")
                    return None
                else:
                    add_log(f"   âš ï¸ HTTP {response.status_code}ï¼Œé‡è¯•ä¸­...")
                    time.sleep(5)
                    
            except requests.exceptions.Timeout:
                add_log(f"   âš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                time.sleep(10)
            except requests.exceptions.ConnectionError:
                add_log(f"   âš ï¸ è¿æ¥é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                time.sleep(10)
            except Exception as e:
                add_log(f"   âš ï¸ è¯·æ±‚é”™è¯¯: {str(e)}")
                time.sleep(5)
        
        add_log(f"   âŒ å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥")
        return None
    
    def download_work(work_url):
        """ä¸‹è½½å•ä¸ªä½œå“"""
        nonlocal saved_count
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
            if is_url_downloaded(work_url):
                add_log(f"â­ï¸ å·²ä¸‹è½½è¿‡ï¼Œè·³è¿‡: {work_url}")
                return
            
            add_log(f"ğŸ“– æ­£åœ¨è·å–: {work_url}")
            
            # å¤„ç† ?view_adult=true å‚æ•°
            if '?' not in work_url:
                work_url_with_adult = work_url + "?view_adult=true"
            else:
                work_url_with_adult = work_url + "&view_adult=true"
            
            # è·å–ä½œå“é¡µé¢ (å¸¦é‡è¯•)
            response = fetch_with_retry(work_url_with_adult)
            if response is None:
                return
            
            html_content = response.content.decode('utf-8')
            soup = BeautifulSoup(html_content, 'html.parser')
            tree = etree.HTML(html_content)
            
            # æå–ä½œå“ä¿¡æ¯ - ä½¿ç”¨BeautifulSoup
            title_elem = soup.find('h2', class_='title heading')
            title = title_elem.get_text(strip=True) if title_elem else "æœªçŸ¥æ ‡é¢˜"
            
            author_elem = soup.find('a', rel='author')
            author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
            
            # è·å–å…ƒæ•°æ®
            metadata = []
            if save_metadata:
                # Fandom
                fandoms = tree.xpath('//dd[@class="fandom tags"]//a/text()')
                if fandoms:
                    metadata.append(f"Fandom: {', '.join(fandoms)}")
                
                # Rating
                rating = tree.xpath('//dd[@class="rating tags"]//a/text()')
                if rating:
                    metadata.append(f"Rating: {rating[0]}")
                
                # Warnings
                warnings = tree.xpath('//dd[@class="warning tags"]//a/text()')
                if warnings:
                    metadata.append(f"Warnings: {', '.join(warnings)}")
                
                # Relationships
                relationships = tree.xpath('//dd[@class="relationship tags"]//a/text()')
                if relationships:
                    metadata.append(f"Relationships: {', '.join(relationships[:5])}")
                
                # Characters
                characters = tree.xpath('//dd[@class="character tags"]//a/text()')
                if characters:
                    metadata.append(f"Characters: {', '.join(characters[:10])}")
                
                # Additional Tags
                tags = tree.xpath('//dd[@class="freeform tags"]//a/text()')
                if tags:
                    metadata.append(f"Tags: {', '.join(tags[:10])}")
                
                # Summary
                summary_elem = tree.xpath('//div[@class="summary module"]//blockquote//text()')
                if summary_elem:
                    summary = ' '.join([s.strip() for s in summary_elem if s.strip()])
                    metadata.append(f"\nSummary:\n{summary}")
                
                # Stats
                words = tree.xpath('//dd[@class="words"]/text()')
                chapters = tree.xpath('//dd[@class="chapters"]/text()')
                if words:
                    metadata.append(f"\nWords: {words[0]}")
                if chapters:
                    metadata.append(f"Chapters: {chapters[0]}")
            
            add_log(f"   ğŸ“ æ ‡é¢˜: {title}")
            add_log(f"   ğŸ‘¤ ä½œè€…: {author}")
            
            # è·å–æ­£æ–‡å†…å®¹
            content_parts = []
            chapters_info = []  # ç”¨äºPDFç”Ÿæˆ: [(ç« èŠ‚æ ‡é¢˜, [æ®µè½åˆ—è¡¨]), ...]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç« èŠ‚
            chapter_links = tree.xpath('//div[@id="chapter_index"]//option/@value')
            
            if chapter_links and download_chapters and len(chapter_links) > 1:
                add_log(f"   ğŸ“‘ å…± {len(chapter_links)} ç« èŠ‚")
                
                for idx, chapter_id in enumerate(chapter_links):
                    chapter_url = f"{work_url.split('?')[0]}/chapters/{chapter_id.split('/')[-1]}?view_adult=true"
                    add_log(f"      ç¬¬ {idx+1}/{len(chapter_links)} ç« ...")
                    task_status['progress'] = int((idx / len(chapter_links)) * 50) + 50
                    
                    try:
                        ch_response = fetch_with_retry(chapter_url)
                        if ch_response is None:
                            continue
                        ch_soup = BeautifulSoup(ch_response.content.decode('utf-8'), 'html.parser')
                        ch_tree = etree.HTML(ch_response.content.decode('utf-8'))
                        
                        # ç« èŠ‚æ ‡é¢˜
                        ch_title_elem = ch_tree.xpath('//h3[@class="title"]//text()')
                        ch_title = ' '.join([t.strip() for t in ch_title_elem if t.strip()])
                        if not ch_title:
                            ch_title = f"ç¬¬ {idx + 1} ç« "
                        
                        # ç« èŠ‚å†…å®¹
                        ch_content_elem = ch_tree.xpath('//div[@class="userstuff module"]//p')
                        ch_content = []
                        for p in ch_content_elem:
                            text = etree.tostring(p, method='text', encoding='unicode')
                            if text.strip():
                                ch_content.append(text.strip())
                        
                        # ä¿å­˜ç« èŠ‚ä¿¡æ¯ç”¨äºPDF
                        chapters_info.append((ch_title, ch_content))
                        
                        # TXTæ ¼å¼
                        if ch_title:
                            content_parts.append(f"\n\n{'='*60}\n{ch_title}\n{'='*60}\n")
                        content_parts.append('\n\n'.join(ch_content))
                        
                        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                        
                    except Exception as e:
                        add_log(f"      âš ï¸ è·å–ç« èŠ‚å¤±è´¥: {str(e)}")
            else:
                # å•ç« èŠ‚æˆ–ä¸ä¸‹è½½å…¨éƒ¨ç« èŠ‚
                content_elem = tree.xpath('//div[@class="userstuff module"]//p | //div[@id="chapters"]//div[@class="userstuff"]//p')
                for p in content_elem:
                    text = etree.tostring(p, method='text', encoding='unicode')
                    if text.strip():
                        content_parts.append(text.strip())
            
            if not content_parts:
                # å°è¯•å…¶ä»–æ–¹å¼è·å–å†…å®¹
                content_elem = tree.xpath('//div[contains(@class, "userstuff")]//text()')
                content_parts = [t.strip() for t in content_elem if t.strip() and len(t.strip()) > 10]
            
            # ç»„è£…TXTæ–‡ç« 
            article = f"{title}\nby {author}\n"
            article += f"åŸæ–‡é“¾æ¥: {work_url}\n"
            article += "\n" + "="*60 + "\n"
            
            if metadata:
                article += "\n".join(metadata)
                article += "\n\n" + "="*60 + "\n"
            
            article += "\n\n".join(content_parts)
            
            # åˆ›å»ºä½œè€…ç›®å½•
            author_dir = os.path.join(base_dir, safe_filename(author))
            os.makedirs(author_dir, exist_ok=True)
            
            # ä¿å­˜TXTæ–‡ä»¶
            txt_filename = f"{safe_filename(title)}.txt"
            txt_filepath = os.path.join(author_dir, txt_filename)
            
            # é¿å…é‡å
            counter = 1
            original_filepath = txt_filepath
            while os.path.exists(txt_filepath):
                name_part = original_filepath.rsplit('.', 1)[0]
                txt_filepath = f"{name_part}({counter}).txt"
                counter += 1
            
            with open(txt_filepath, 'w', encoding='utf-8') as f:
                f.write(article)
            
            saved_count += 1
            add_log(f"   âœ… å·²ä¿å­˜: {txt_filename}")
            
            # è®°å½•åˆ°ä¸‹è½½å†å²
            add_to_history(
                item_type='ao3',
                url=work_url,
                title=title,
                author=author,
                file_path=txt_filepath,
                source='ao3'
            )
            
            # å¦‚æœéœ€è¦å¯¼å‡ºPDF
            if export_pdf:
                add_log(f"   ğŸ“„ æ­£åœ¨ç”ŸæˆPDF...")
                pdf_filename = txt_filename.replace('.txt', '.pdf')
                pdf_filepath = txt_filepath.replace('.txt', '.pdf')
                
                # ç”ŸæˆHTMLå†…å®¹
                html_content = generate_html_content(
                    title=title,
                    author=author,
                    work_url=work_url,
                    metadata_list=metadata,
                    content_parts=content_parts if not chapters_info else [],
                    chapters_info=chapters_info if chapters_info else None
                )
                
                # åŒæ—¶ä¿å­˜HTMLæ–‡ä»¶ï¼ˆæ–¹ä¾¿è°ƒè¯•å’Œè‡ªå®šä¹‰ï¼‰
                html_filepath = txt_filepath.replace('.txt', '.html')
                with open(html_filepath, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                # ç”ŸæˆPDF
                if save_as_pdf(html_content, pdf_filepath):
                    add_log(f"   ğŸ“„ å·²ç”ŸæˆPDF: {pdf_filename}")
            
            # å¦‚æœéœ€è¦å¯¼å‡ºEPUB
            if export_epub:
                add_log(f"   ğŸ“– æ­£åœ¨ç”ŸæˆEPUB...")
                epub_filename = txt_filename.replace('.txt', '.epub')
                epub_filepath = txt_filepath.replace('.txt', '.epub')
                
                if generate_epub(
                    title=title,
                    author=author,
                    content_parts=content_parts,
                    chapters_info=chapters_info if chapters_info else None,
                    metadata_list=metadata,
                    filepath=epub_filepath
                ):
                    add_log(f"   ğŸ“– å·²ç”ŸæˆEPUB: {epub_filename}")
            
        except Exception as e:
            add_log(f"   âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
    
    def get_works_from_series(series_url):
        """è·å–ç³»åˆ—ä¸­çš„æ‰€æœ‰ä½œå“é“¾æ¥"""
        try:
            add_log(f"ğŸ“š è·å–ç³»åˆ—ä½œå“åˆ—è¡¨: {series_url}")
            response = fetch_with_retry(series_url)
            if response is None:
                return []
            tree = etree.HTML(response.content.decode('utf-8'))
            
            work_links = tree.xpath('//ul[@class="series work index group"]//h4[@class="heading"]//a[1]/@href')
            work_urls = [f"https://archiveofourown.org{link}" for link in work_links if '/works/' in link]
            
            add_log(f"   æ‰¾åˆ° {len(work_urls)} ç¯‡ä½œå“")
            return work_urls
        except Exception as e:
            add_log(f"   âŒ è·å–ç³»åˆ—å¤±è´¥: {str(e)}")
            return []
    
    def get_works_from_author(author_url, max_pages=20):
        """è·å–ä½œè€…çš„æ‰€æœ‰ä½œå“é“¾æ¥"""
        try:
            add_log(f"ğŸ‘¤ è·å–ä½œè€…ä½œå“åˆ—è¡¨: {author_url}")
            
            all_works = []
            page = 1
            
            while True:
                page_url = f"{author_url}?page={page}"
                response = fetch_with_retry(page_url)
                if response is None:
                    break
                tree = etree.HTML(response.content.decode('utf-8'))
                
                work_links = tree.xpath('//ol[@class="work index group"]//h4[@class="heading"]//a[1]/@href')
                new_works = [f"https://archiveofourown.org{link}" for link in work_links if '/works/' in link]
                
                if not new_works:
                    break
                
                all_works.extend(new_works)
                add_log(f"   ç¬¬ {page} é¡µ: æ‰¾åˆ° {len(new_works)} ç¯‡")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page = tree.xpath('//li[@class="next"]//a/@href')
                if not next_page:
                    break
                
                page += 1
                if page > max_pages:
                    add_log(f"   âš ï¸ å·²è¾¾åˆ° {max_pages} é¡µé™åˆ¶")
                    break
                
                time.sleep(0.5)
            
            add_log(f"   å…±æ‰¾åˆ° {len(all_works)} ç¯‡ä½œå“")
            return all_works
            
        except Exception as e:
            add_log(f"   âŒ è·å–ä½œè€…ä½œå“å¤±è´¥: {str(e)}")
            return []
    
    def get_works_from_tag(tag_url, max_pages=5):
        """è·å–Tagä¸‹çš„æ‰€æœ‰ä½œå“é“¾æ¥"""
        try:
            # æå–tagåç§°ç”¨äºæ˜¾ç¤º
            tag_match = re.search(r'/tags/([^/]+)/works', tag_url)
            tag_name = tag_match.group(1) if tag_match else "æœªçŸ¥æ ‡ç­¾"
            tag_name = requests.utils.unquote(tag_name)
            
            add_log(f"ğŸ·ï¸ è·å–Tagä½œå“åˆ—è¡¨: {tag_name}")
            
            all_works = []
            page = 1
            
            while True:
                # AO3 tagé¡µé¢çš„åˆ†é¡µæ ¼å¼
                if '?' in tag_url:
                    page_url = f"{tag_url}&page={page}"
                else:
                    page_url = f"{tag_url}?page={page}"
                
                add_log(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
                response = fetch_with_retry(page_url)
                
                if response is None:
                    add_log(f"   âš ï¸ è·å–é¡µé¢å¤±è´¥")
                    break
                
                tree = etree.HTML(response.content.decode('utf-8'))
                
                # AO3 ä½œå“åˆ—è¡¨çš„é€‰æ‹©å™¨
                work_links = tree.xpath('//ol[contains(@class, "work index")]//h4[@class="heading"]//a[1]/@href')
                new_works = [f"https://archiveofourown.org{link}" for link in work_links if '/works/' in link]
                
                if not new_works:
                    add_log(f"   ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šä½œå“")
                    break
                
                all_works.extend(new_works)
                add_log(f"   ç¬¬ {page} é¡µ: æ‰¾åˆ° {len(new_works)} ç¯‡")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page = tree.xpath('//li[@class="next"]//a/@href')
                if not next_page:
                    add_log("   å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                
                page += 1
                if page > max_pages:
                    add_log(f"   âš ï¸ å·²è¾¾åˆ° {max_pages} é¡µé™åˆ¶")
                    break
                
                time.sleep(1)  # AO3å¯¹é¢‘ç¹è¯·æ±‚æ¯”è¾ƒæ•æ„Ÿ
            
            add_log(f"   ğŸ·ï¸ Tag [{tag_name}] å…±æ‰¾åˆ° {len(all_works)} ç¯‡ä½œå“")
            return all_works
            
        except Exception as e:
            add_log(f"   âŒ è·å–Tagä½œå“å¤±è´¥: {str(e)}")
            return []
    
    # è·å–æœ€å¤§é¡µæ•°å‚æ•°
    max_pages = params.get('max_pages', 5)
    
    # å¤„ç†æ¯ä¸ªURL
    all_work_urls = []
    
    for url in urls:
        url = url.strip()
        if not url:
            continue
        
        if '/series/' in url:
            # ç³»åˆ—ä½œå“
            work_urls = get_works_from_series(url)
            all_work_urls.extend(work_urls)
        elif '/users/' in url and '/works' in url:
            # ä½œè€…ä½œå“é¡µ
            work_urls = get_works_from_author(url, max_pages)
            all_work_urls.extend(work_urls)
        elif '/tags/' in url and '/works' in url:
            # Tagä½œå“é¡µ
            work_urls = get_works_from_tag(url, max_pages)
            all_work_urls.extend(work_urls)
        elif '/works/' in url:
            # å•ä¸ªä½œå“
            all_work_urls.append(url)
        else:
            add_log(f"âš ï¸ æ— æ³•è¯†åˆ«çš„é“¾æ¥æ ¼å¼: {url}")
    
    # å»é‡
    all_work_urls = list(dict.fromkeys(all_work_urls))
    add_log(f"ğŸ“Š å…± {len(all_work_urls)} ç¯‡ä½œå“å¾…ä¸‹è½½")
    
    # ä¸‹è½½æ‰€æœ‰ä½œå“
    for idx, work_url in enumerate(all_work_urls):
        task_status['progress'] = int((idx / len(all_work_urls)) * 100)
        download_work(work_url)
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    add_log(f"âœ… AO3ä¸‹è½½å®Œæˆï¼")
    add_log(f"   ğŸ“š å…±ä¿å­˜ {saved_count} ç¯‡æ–‡ç« ")
    add_log(f"   ğŸ“ ä¿å­˜ä½ç½®: {base_dir}/ä½œè€…å/")


# ============ API è·¯ç”± ============

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')

@app.route('/api/config', methods=['GET', 'POST'])
def handle_config():
    """å¤„ç†é…ç½®"""
    if request.method == 'GET':
        cfg = load_config()
        hidden_auth = cfg['login_auth']
        if len(hidden_auth) > 10:
            hidden_auth = hidden_auth[:5] + '...' + hidden_auth[-5:]
        return jsonify({
            'login_key': cfg['login_key'],
            'login_auth': hidden_auth,
            'has_auth': bool(cfg['login_auth']),
            'file_path': cfg['file_path']
        })
    else:
        data = request.json
        save_login_info(data.get('login_key', ''), data.get('login_auth', ''))
        return jsonify({'success': True, 'message': 'é…ç½®å·²ä¿å­˜'})

@app.route('/api/task/start', methods=['POST'])
def start_task():
    """å¯åŠ¨ä»»åŠ¡"""
    if task_status['running']:
        return jsonify({'success': False, 'message': 'å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œä¸­'})
    
    data = request.json
    task_type = data.get('type')
    params = data.get('params', {})
    
    thread = threading.Thread(target=run_spider_task, args=(task_type, params))
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²å¯åŠ¨'})

@app.route('/api/task/status')
def get_task_status():
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    return jsonify(task_status)

@app.route('/api/task/stop', methods=['POST'])
def stop_task():
    """åœæ­¢ä»»åŠ¡"""
    task_status['running'] = False
    add_log('âš ï¸ ä»»åŠ¡å·²åœæ­¢')
    return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²åœæ­¢'})

@app.route('/api/files')
def list_files():
    """åˆ—å‡ºå·²ä¸‹è½½çš„æ–‡ä»¶"""
    base_path = config['file_path']
    files = []
    
    if os.path.exists(base_path):
        for root, dirs, filenames in os.walk(base_path):
            for filename in filenames:
                if not filename.endswith('.json') and not filename.startswith('.'):
                    full_path = os.path.join(root, filename)
                    rel_path = os.path.relpath(full_path, base_path)
                    file_size = os.path.getsize(full_path)
                    files.append({
                        'name': filename,
                        'path': rel_path,
                        'size': file_size,
                        'type': 'image' if filename.lower().endswith(('.jpg', '.png', '.gif', '.jpeg')) else 'text'
                    })
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    files.sort(key=lambda x: x['name'], reverse=True)
    return jsonify({'files': files[:200], 'total': len(files)})

@app.route('/static/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory('static', filename)

# ============ ä¸‹è½½å†å² API ============

@app.route('/api/history')
def get_history():
    """è·å–ä¸‹è½½å†å²"""
    history = load_download_history()
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    filter_type = request.args.get('type', '')  # 'image', 'article', 'ao3', ''
    filter_source = request.args.get('source', '')  # 'lofter', 'ao3', ''
    search = request.args.get('search', '')
    
    items = history['items']
    
    # è¿‡æ»¤
    if filter_type:
        items = [i for i in items if i.get('type') == filter_type]
    if filter_source:
        items = [i for i in items if i.get('source') == filter_source]
    if search:
        search_lower = search.lower()
        items = [i for i in items if 
                 search_lower in i.get('title', '').lower() or 
                 search_lower in i.get('author', '').lower()]
    
    # åˆ†é¡µ
    total = len(items)
    start = (page - 1) * per_page
    end = start + per_page
    items = items[start:end]
    
    return jsonify({
        'items': items,
        'total': total,
        'page': page,
        'per_page': per_page,
        'total_pages': (total + per_page - 1) // per_page,
        'stats': history['stats']
    })

@app.route('/api/history/clear', methods=['POST'])
def api_clear_history():
    """æ¸…ç©ºä¸‹è½½å†å²"""
    result = clear_download_history()
    if result:
        return jsonify({'success': True, 'message': 'å†å²è®°å½•å·²æ¸…ç©º'})
    return jsonify({'success': False, 'message': 'æ¸…ç©ºå¤±è´¥'})

@app.route('/api/history/delete/<item_id>', methods=['DELETE'])
def delete_history_item(item_id):
    """åˆ é™¤å•æ¡å†å²è®°å½•"""
    history = load_download_history()
    history['items'] = [i for i in history['items'] if i.get('id') != item_id]
    save_download_history(history)
    return jsonify({'success': True, 'message': 'è®°å½•å·²åˆ é™¤'})

@app.route('/api/history/check', methods=['POST'])
def check_downloaded():
    """æ£€æŸ¥URLæ˜¯å¦å·²ä¸‹è½½"""
    data = request.json
    url = data.get('url', '')
    downloaded = is_url_downloaded(url)
    return jsonify({'downloaded': downloaded, 'url': url})

@app.route('/api/settings', methods=['GET', 'POST'])
def handle_settings():
    """å¤„ç†åº”ç”¨è®¾ç½®"""
    global config
    if request.method == 'GET':
        # ç¡®ä¿åŠ è½½æœ€æ–°é…ç½®
        load_config_file()
        return jsonify({
            'save_path': config.get('save_path', './dir'),
            'dark_mode': config.get('dark_mode', False),
            'auto_dedup': config.get('auto_dedup', True),
            'notify_on_complete': config.get('notify_on_complete', True)
        })
    else:
        data = request.json
        if 'save_path' in data:
            save_path = data['save_path']
            config['save_path'] = save_path
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            try:
                os.makedirs(save_path, exist_ok=True)
                os.makedirs(os.path.join(save_path, 'img'), exist_ok=True)
                os.makedirs(os.path.join(save_path, 'article'), exist_ok=True)
            except Exception as e:
                return jsonify({'success': False, 'message': f'åˆ›å»ºç›®å½•å¤±è´¥: {str(e)}'})
        if 'dark_mode' in data:
            config['dark_mode'] = data['dark_mode']
        if 'auto_dedup' in data:
            config['auto_dedup'] = data['auto_dedup']
        if 'notify_on_complete' in data:
            config['notify_on_complete'] = data['notify_on_complete']
        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        save_config_file()
        return jsonify({'success': True, 'message': 'è®¾ç½®å·²ä¿å­˜'})

if __name__ == '__main__':
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    load_config()
    
    save_path = config.get('save_path', './dir')
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(os.path.join(save_path, 'img'), exist_ok=True)
    os.makedirs(os.path.join(save_path, 'article'), exist_ok=True)
    
    print("=" * 50)
    print("Lofter Spider Web Application")
    print("=" * 50)
    print(f"ä¿å­˜è·¯å¾„: {save_path}")
    print("Visit http://localhost:5000 to start")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)
